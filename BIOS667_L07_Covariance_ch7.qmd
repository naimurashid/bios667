---
title: "BIOS 667 — Lecture 7: Modeling the Covariance (Ch. 7)"
subtitle: "Fitzmaurice, Laird & Ware (2011) — Applied Longitudinal Data Analysis"
author: "Naim Rashid"
format:
  revealjs:
    theme: [default]
    footer: BIOS 667 · UNC Gillings — Lecture 7 (Ch.7)
    slide-number: true
    hash: true
    toc: false
    code-overflow: wrap
    code-line-numbers: false
    math: mathjax
    incremental: true
    embed-resources: true
    chalkboard: false
    css: "unc-gillings.css"
    scrollable: true
execute:
  echo: false
  warning: false
  message: false
---

## 7.1–7.2 Introduction & Implications  

- Longitudinal data have **within-person correlation** that we must model (Σ).
- Correct Σ ⇒ **valid SEs** and **more power**; mis-specified Σ ⇒ misleading inferences.
- Mean model and covariance model are **interdependent** (residuals depend on the mean).
- Positive correlation is **useful**: it lowers the variance of **within-person changes**.
- With **MAR** missingness, valid likelihood inference needs a sensible **mean + Σ**.

---

## Core math  

- Paired “before–after” change (two time points):
  $$
  \operatorname{Var}(Y_{i2}-Y_{i1})
  \;=\; \sigma_1^2+\sigma_2^2-2\rho_{12}\sigma_1\sigma_2.
  $$
- If $\sigma_1^2=\sigma_2^2=\sigma^2$, then
  $$
  \operatorname{Var}(Y_{i2}-Y_{i1}) \;=\; 2\sigma^2(1-\rho).
  $$
- Cross-sectional difference between independent people: $2\sigma^2$.  
  **Ratio** (within / between) $=\;1-\rho$.

---

<!-- Placement: Right after “Core math” slide -->

## Why Σ changes SEs 

- For within-person change $\Delta=Y_{i2}-Y_{i1}$ with $\sigma^2=1$,
  $\mathrm{Var}(\Delta)=2\sigma^2(1-\rho)=2(1-\rho).$
  - Higher $\rho$ ⇒ smaller $$\mathrm{SE}(\Delta)$$ ⇒ more power for change.

```{r}
#| echo: true
#| output-location: slide
#| 
data.frame(rho = c(0, 0.3, 0.7)) |>
  dplyr::mutate(Var_Delta = 2*(1-rho),
                SE_Delta  = sqrt(Var_Delta))
                
```


## GLS demo with simulated data 

- **Independence** ignores both serial correlation and heteroscedasticity ⇒ usually worst AIC.  
- **AR(1) only** captures serial correlation (off-diagonals) but keeps a constant variance.  
- **varIdent only** captures changing variance by time (diagonal) but treats residuals as independent.  
- **ARH(1)** (AR(1)+varIdent) handles **both**, and typically fits best when data truly have **serial correlation** *and* **time-varying variance**.


## GLS demo with simulated data 

```{r}
#| echo: true
#| output-location: slide
set.seed(667)
library(dplyr)
library(tidyr)
library(ggplot2)
library(nlme)

# ------------------------------
# Simulate balanced longitudinal data with AR(1) correlation + heteroscedasticity over time
# ------------------------------
N   <- 120           # subjects
m   <- 5             # visits
times <- 0:4
rho_true <- 0.6      # true serial correlation
sigma_true <- 1.2    # marginal SD scale for the AR(1) part
omega <- c(1.0, 1.1, 1.5, 1.2, 0.9)  # time-specific SD multipliers (heteroscedasticity)

# AR(1) generator with stationary Var = 1 (so multipliers control hetero cleanly)
sim_ar1_std <- function(m, rho){
  z <- numeric(m)
  z[1] <- rnorm(1, sd = 1)                # Var(z1)=1
  if (m > 1) {
    eps <- rnorm(m - 1, sd = sqrt(1 - rho^2))
    for (t in 2:m) z[t] <- rho * z[t - 1] + eps[t - 1]
  }
  z
}

ids <- factor(seq_len(N))
design <- expand.grid(id = ids, time_idx = seq_len(m)) |>
  arrange(id, time_idx) |>
  mutate(time_f   = factor(time_idx, levels = seq_len(m),
                           labels = paste0("t", times)),
         time_num = times[time_idx])

# Mean structure: linear time trend + small group effect (not essential here)
beta0 <- 10
beta_time <- 0.8
group <- factor(rep(c("A","B"), each = N/2))
design <- design |>
  group_by(id) |>
  mutate(group = group[as.integer(id)]) |>
  ungroup()

# Subject random intercept (to mimic between-person heterogeneity)
sigma_b <- 1.0
b_i <- rnorm(N, 0, sigma_b)
b <- b_i[as.integer(design$id)]

# AR(1) errors, then apply time-specific SD multipliers (varIdent idea)
z <- do.call(c, lapply(seq_len(N), function(i) sim_ar1_std(m, rho_true)))
err <- sigma_true * (omega[design$time_idx]) * z

sim <- design |>
  mutate(mu = beta0 + beta_time * time_num + ifelse(group=="B", 0.5, 0),
         y  = mu + b + err)

# ------------------------------
# Fit GLS under:
#   (1) Independence (no correlation, no hetero)
#   (2) AR(1) correlation only
#   (3) varIdent (hetero) only
#   (4) ARH(1): AR(1) + varIdent
# ------------------------------
fit_ind <- gls(y ~ time_num, data = sim, method = "REML")

fit_ar1 <- gls(y ~ time_num, data = sim,
               correlation = corAR1(form = ~ time_idx | id),
               method = "REML")

# Initialize varIdent with numeric starting values (robust practice)
w_time <- varIdent(form = ~ 1 | time_f)
nlme::coef(w_time) <- setNames(rep(1, nlevels(sim$time_f) - 1),
                               levels(sim$time_f)[-1])

fit_vhet <- gls(y ~ time_num, data = sim,
                weights = w_time,
                method  = "REML")

fit_arh1 <- gls(y ~ time_num, data = sim,
                correlation = corAR1(form = ~ time_idx | id),
                weights     = w_time,
                method      = "REML")

# ------------------------------
# Compare fits and extract key parameters
# ------------------------------
tab_ic <- data.frame(
  model  = c("Independence","AR(1) only","varIdent only","ARH(1) = AR(1)+varIdent"),
  AIC    = c(AIC(fit_ind), AIC(fit_ar1), AIC(fit_vhet), AIC(fit_arh1)),
  BIC    = c(BIC(fit_ind), BIC(fit_ar1), BIC(fit_vhet), BIC(fit_arh1)),
  logLik = c(logLik(fit_ind), logLik(fit_ar1), logLik(fit_vhet), logLik(fit_arh1))
) |>
  arrange(AIC)

# Estimated AR(1) correlation (rho)
rho_hat_ar1  <- as.numeric(coef(fit_ar1$modelStruct$corStruct))
rho_hat_arh1 <- as.numeric(coef(fit_arh1$modelStruct$corStruct))

# Estimated varIdent SD multipliers by time (baseline level fixed at 1)
vi_coef <- coef(fit_arh1$modelStruct$varStruct, unconstrained = FALSE)
levs <- levels(sim$time_f)
vi_all <- setNames(rep(1, length(levs)), levs)
vi_all[names(vi_coef)] <- vi_coef

# Turn multipliers into estimated residual SDs = sigma * multiplier
sigma_eps <- sigma(fit_arh1)
sd_time <- sigma_eps * vi_all
sd_table <- data.frame(time = names(sd_time), sd_est = as.numeric(sd_time))

# ------------------------------
# Print key outputs to the slides
# ------------------------------
tab_ic
data.frame(
  param = c("rho_hat (AR1 only)","rho_hat (ARH1)"),
  estimate = c(rho_hat_ar1, rho_hat_arh1)
)

data.frame(
  note = "varIdent SD multipliers (baseline level = 1.0)",
  time = names(vi_all),
  multiplier = as.numeric(vi_all)
)

sd_table

# Optional quick visual: estimated residual SD by time under ARH(1)
ggplot(sd_table, aes(x = time, y = sd_est, group = 1)) +
  geom_line() + geom_point(size = 2) +
  labs(title = "Estimated residual SD by time (GLS with AR(1) + varIdent)",
       x = "Time (factor levels)", y = "Residual SD") +
  theme_minimal()
```



                
## TLC trial → long format; add index for Σ

```{r}
#| echo: true
#| output-location: slide
library(dplyr)
library(tidyr)
library(nlme)
library(emmeans)
library(ggplot2)

# Read attached file (short names)
path <- "data/tlc.csv"
tlc_wide <- read.csv(path, check.names = FALSE, stringsAsFactors = FALSE)
names(tlc_wide) <- trimws(names(tlc_wide))

tlc_wide <- tlc_wide %>%
  rename(
    id  = ID,
    trt = `Treatment Group`,
    w0  = `Lead Level Week 0`,
    w1  = `Lead Level Week 1`,
    w4  = `Lead Level Week 4`,
    w6  = `Lead Level Week 6`
  ) %>%
  mutate(trt = factor(trt, levels = c("P","A"), labels = c("Placebo","Succimer")))

# Wide -> long; time as factor (ordered), numeric, and index (1..m) for corSymm
long <- tlc_wide %>%
  pivot_longer(c(w0,w1,w4,w6), names_to = "wk", values_to = "y") %>%
  mutate(
    time       = recode(wk, w0="0", w1="1", w4="4", w6="6"),
    time_f     = factor(time, levels = c("0","1","4","6")),
    time_num   = as.numeric(as.character(time)),
    time_idx   = as.integer(time_f),
    group      = trt,
    id         = factor(id),
    y          = as.numeric(y)
  ) %>%
  select(id, group, time_f, time_num, time_idx, y) %>%
  arrange(id, time_idx)

# Quick sanity checks
stopifnot(all(!is.na(long$y)), all(!is.na(long$time_f)), all(!is.na(long$time_idx)))

# Helpers used later (stable varIdent init; UN start vector length)
make_varIdent <- function(data, fac) {
  f <- nlme::varIdent(form = as.formula(paste0("~ 1 | ", fac)))
  levs <- levels(data[[fac]])
  if (length(levs) > 1) nlme::coef(f) <- stats::setNames(rep(1, length(levs) - 1), levs[-1])
  f
}
w_time <- make_varIdent(long, "time_f")

m_visits <- nlevels(long$time_f)
p_lower  <- m_visits * (m_visits - 1) / 2
start_vec_un <- rep(0.2, p_lower)

head(long)
```

---

## Equal-spacing check & time scaling tip

```{r}
#| echo: true
#| output-location: slide
# Check planned time spacing and consider unit scaling
planned <- levels(long$time_f)
gaps <- diff(sort(unique(long$time_num)))
data.frame(planned_times = planned, unique_gaps = paste(unique(gaps), collapse = ", "))

# Tip: continuous-time models depend on time units; consider scaling for interpretability
# Example: weeks -> months (optional)
# long <- long |> dplyr::mutate(time_scaled = time_num / 4)
```



## Empirical check: correlation & the 1−ρ index

```{r}
#| echo: true
#| output-location: slide
# Compute rho at baseline vs week 1 (paired within person)
bl_w1 <- tlc_wide %>% select(id, trt, w0, w1) %>% filter(!is.na(w0) & !is.na(w1))
rho_hat <- with(bl_w1, cor(w0, w1))

var_within_hat   <- var(bl_w1$w1 - bl_w1$w0)
var_between_hat  <- var(bl_w1$w1) + var(bl_w1$w0)
ratio_hat <- var_within_hat / var_between_hat

data.frame(
  rho_hat = round(rho_hat, 3),
  ratio_within_to_between = round(ratio_hat, 3),
  approx_1_minus_rho = round(1 - rho_hat, 3)
)
```

---

## Diagnostic: Residual ACF

- Tweaks to make the contrast **clearer**:
  - More subjects (**N = 400**) and visits (**m = 8**) ⇒ better estimation of Σ.
  - Stronger true serial correlation (**ρ = 0.8**) ⇒ Independence leaves larger residual ACF.
- Expectation: **AR(1) fit** (correct Σ) ≈ **flat residual ACF**, Independence shows **positive lag-1 ACF**.

```{r}
#| echo: true
#| output-location: slide
set.seed(667)
library(dplyr)
library(tidyr)
library(ggplot2)
library(nlme)

# ------------------------------
# Simulate longitudinal data with AR(1) errors
# ------------------------------
N   <- 400      # more subjects
m   <- 8        # more visits
rho <- 0.80     # stronger serial correlation
sigma <- 2      # marginal SD for the AR(1) error process

# AR(1) generator with stationary Var = sigma^2
sim_ar1 <- function(m, rho, sigma){
  e <- numeric(m)
  e[1] <- rnorm(1, sd = sigma)
  if (m > 1) {
    w <- rnorm(m - 1, sd = sigma * sqrt(1 - rho^2))
    for (t in 2:m) e[t] <- rho * e[t - 1] + w[t - 1]
  }
  e
}

ids   <- factor(seq_len(N))
times <- seq_len(m)

design <- expand.grid(id = ids, time_idx = times) |>
  arrange(id, time_idx) |>
  mutate(
    group    = ifelse(as.integer(id) <= N/2, "A", "B"),
    group    = factor(group, levels = c("A","B")),
    time_f   = factor(time_idx, levels = times),
    time_num = as.numeric(time_idx)
  )

# Fixed mean structure (rich enough to avoid mean misspecification)
beta0 <- 10
beta_time <- 0.7
beta_groupB <- 1.0
beta_int_B <- 0.3

errs <- do.call(c, lapply(seq_len(N), function(i) sim_ar1(m, rho, sigma)))

sim_long <- design |>
  mutate(
    mu = beta0 +
         beta_time   * (time_num - 1) +
         beta_groupB * as.numeric(group == "B") +
         beta_int_B  * as.numeric(group == "B") * (time_num - 1),
    y  = mu + errs
  )

# ------------------------------
# Fit GLS under Independence and AR(1)
# ------------------------------
gls_ind <- gls(y ~ group * time_f, data = sim_long, method = "REML")

gls_ar1 <- gls(y ~ group * time_f, data = sim_long,
               correlation = corAR1(form = ~ time_idx | id),
               method = "REML")

# Attach normalized residuals (balanced -> 1:1 match)
res_ind <- sim_long |> mutate(res = residuals(gls_ind, type = "normalized"))
res_ar1 <- sim_long |> mutate(res = residuals(gls_ar1, type = "normalized"))

# ------------------------------
# Pooled residual ACF by subject (lags 1–3)
# ------------------------------
lag_cor <- function(x, k){
  n <- length(x); if (n <= k) return(NA_real_)
  stats::cor(x[(k+1):n], x[1:(n-k)], use = "pairwise.complete.obs")
}

acf_by_id <- function(dat){
  dat |>
    arrange(id, time_idx) |>
    group_by(id) |>
    summarise(acf1 = lag_cor(res, 1),
              acf2 = lag_cor(res, 2),
              acf3 = lag_cor(res, 3),
              .groups = "drop")
}

pool_acf <- function(dat, model_lab){
  by_id <- acf_by_id(dat)
  by_id |>
    tidyr::pivot_longer(cols = dplyr::starts_with("acf"),  # <-- FIX: pivot only acf* columns
                        names_to = "lag", values_to = "rho") |>
    mutate(lag = as.integer(sub("acf", "", lag))) |>
    group_by(lag) |>
    summarise(
      mean_rho = mean(rho, na.rm = TRUE),
      med_rho  = median(rho, na.rm = TRUE),
      se_rho   = sd(rho, na.rm = TRUE)/sqrt(dplyr::n()),
      .groups  = "drop"
    ) |>
    mutate(model = model_lab)
}

acf_ind <- pool_acf(res_ind, "Independence")
acf_ar1 <- pool_acf(res_ar1, "AR(1)")

acf_all <- bind_rows(acf_ind, acf_ar1)

# ------------------------------
# Visualize: bars = mean pooled ACF; error bars = ±2 SE
# ------------------------------
ggplot(acf_all, aes(x = factor(lag), y = mean_rho, fill = model)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.6) +
  geom_errorbar(aes(ymin = mean_rho - 2*se_rho, ymax = mean_rho + 2*se_rho),
                position = position_dodge(width = 0.7), width = 0.15) +
  geom_hline(yintercept = 0, linewidth = 0.3) +
  labs(title = "Pooled residual ACF by model (simulated AR(1) truth, ρ = 0.80)",
       subtitle = "Correct Σ (AR(1)) ≈ flat residual ACF; Independence leaves clear positive lag-1 correlation",
       x = "Lag", y = "Residual correlation") +
  theme_minimal()

# Optional: quick check of fitted AR(1) parameter (should be close to 0.8)
coef(gls_ar1$modelStruct$corStruct)
```

## Diagnostic: Empirical Semivariogram

- A **variogram** is a plot of residual variance versus time separation.
- It helps visualize the correlation structure.
  - **Flat variogram:** Suggests Compound Symmetry might be reasonable.
  - **Increasing variogram:** Suggests correlation decays with time (e.g., AR(1), Exponential).
- We can plot the empirical variogram from our model residuals to check the fit.

```{r}
#| echo: true
#| output-location: slide
# The nlme package has a built-in Variogram function 
# that works directly with gls objects.
library(nlme)

# Calculate the empirical variogram from the independence model's residuals.
# This helps visualize the covariance structure that we need to model.
vario_gls <- Variogram(gls_ind, form = ~ time_num)

# The plot method for Variogram objects is well-formatted.
plot(vario_gls, main = "Empirical Semivariogram of Residuals", 
     xlab = "Time Lag (Weeks)", ylab = "Semivariance")

```

##  Σ changes **SEs for within-group change**

- Same **mean model** ($y \sim \text{group} \times \text{time}_f$).
- Compare Σ via `gls`: **Independence**, **AR(1)**, **UN (hetero)**.
- Extract **within-group change** (week 1 − baseline) for **each group** with **SEs** using `emmeans`.  
 

```{r}
#| echo: true
#| output-location: slide
# Fit three Σ choices with the same mean
gls_ind <- gls(y ~ group * time_f, data = long, method = "REML")

gls_ar1 <- gls(y ~ group * time_f, data = long,
               correlation = corAR1(form = ~ time_idx | id), method = "REML")

gls_un  <- gls(y ~ group * time_f, data = long,
               correlation = corSymm(value = start_vec_un, form = ~ time_idx | id),
               weights     = w_time,
               method = "REML")

# Helper: within-group change (week1 - baseline) via emmeans
get_change_tbl <- function(fit){
  emm_t_by_g <- emmeans(fit, ~ time_f | group)
  contr <- contrast(emm_t_by_g,
                    method = list("week1 - baseline" = c(-1, 1, 0, 0)))
  as.data.frame(summary(contr))[, c("group","estimate","SE","df","t.ratio","p.value")]
}

tab_ind <- get_change_tbl(gls_ind); tab_ind$model <- "Independence"
tab_ar1 <- get_change_tbl(gls_ar1); tab_ar1$model <- "AR(1)"
tab_un  <- get_change_tbl(gls_un ); tab_un$model  <- "UN (hetero)"

tab_all <- bind_rows(tab_ind, tab_ar1, tab_un) %>%
  select(model, group, estimate, SE, df, t.ratio, p.value)

tab_all
```

---

## Common errors quick-ref  


- `Initialize.corSymm`: wrong dimension → start vector length (value) must equal \(m(m-1)/2\).
- “unique values must be consecutive integers” → ensure `time_idx = as.integer(time_f)`.
- `abs(value) non-numeric` → `varIdent` coefficients must be numeric (not characters/factors).

---

## Reading the table  

- **Estimates** should be similar across Σ if the mean model is sensible.
- **SEs** shrink under a realistic Σ (AR(1) or UN) compared to **Independence**.
- **Under Independence** SEs for within-person change are too large—you leave power on the table
- **Takeaway:** modeling Σ **recovers precision** for change contrasts (no duplicate plots).

---

## MAR missingness: why Σ matters for validity

- With **MAR** (not MCAR), Likelihood-based inference under MAR is valid if the joint distribution (mean + Σ) is correctly specified
- If Σ is badly misspecified, SEs and tests for change can be **misleading**.
- Practice: start with a **rich mean** (time as factor + key interactions), then choose Σ; report sensitivity when feasible.

---


## Strategy recap  

- Model Σ because it **alters SEs and power** and is necessary under **MAR**.
- Don’t re-plot the same idea: we now use a **contrast table** to show efficiency gains.
- Next up: how to **specify Σ** — **Unstructured (7.3)** and **Covariance-pattern (7.4)** — with diagnostics to guide the choice.

## 7.3 Unstructured Covariance (Σ = UN)

- Let $\Sigma$ be **arbitrary** (symmetric, positive-definite).
- $n$ visits ⇒ parameters $= n(n+1)/2$ (e.g., $n{=}3\Rightarrow 6$, $n{=}5\Rightarrow 15$, $n{=}10\Rightarrow 55$).
- Pros: **flexible** (incl. unequal variances by time).
- Cons: many parameters; needs large $N$; awkward for **mistimed** visits.

---



## Fit Σ candidates (UN-het, CS, AR(1), EXP, plus variants)

```{r}
#| echo: true
#| output-location: slide
# UN with heterogeneous variances (corSymm needs consecutive time index + start values)
gls_un <- gls(y ~ group * time_f, data = long,
              correlation = corSymm(value = start_vec_un, form = ~ time_idx | id),
              weights     = w_time,
              method      = "REML")

# Compound Symmetry (homoscedastic)
gls_cs <- gls(y ~ group * time_f, data = long,
              correlation = corCompSymm(form = ~ 1 | id),
              method      = "REML")

# AR(1) with equal spacing (use integer index)
gls_ar1 <- gls(y ~ group * time_f, data = long,
               correlation = corAR1(form = ~ time_idx | id),
               method      = "REML")

# Exponential (continuous-time; handles unequal spacing via time_num)
gls_exp <- gls(y ~ group * time_f, data = long,
               correlation = corExp(form = ~ time_num | id),
               method      = "REML")

# Exponential with heterogeneous variances by time
gls_exp_het <- gls(y ~ group * time_f, data = long,
                   correlation = corExp(form = ~ time_num | id),
                   weights     = w_time,
                   method      = "REML")

# Exponential + nugget (replicates at same time have corr < 1)
gls_exp_nug <- gls(y ~ group * time_f, data = long,
                   correlation = corExp(form = ~ time_num | id, nugget = TRUE),
                   method      = "REML")
```

---




## Parameter Explosion with UN

```{r}
#| echo: true
#| output-location: slide
n <- 2:10
params_UN <- n*(n+1)/2
params_AR1 <- rep(2, length(n))
params_CS  <- rep(2, length(n))
df <- data.frame(n, UN=params_UN, AR1=params_AR1, CS=params_CS) |>
  tidyr::pivot_longer(-n, names_to="model", values_to="p")

ggplot(df, aes(n, p, color=model)) +
  geom_line() + geom_point() +
  labs(title="Number of covariance parameters vs visits",
       x="Number of visits (n)", y="Number of parameters") +
  theme_minimal()
```

---

## Parameter growth vs usable N (sanity check)

*(Rule-of-thumb: be wary if N / #cov-params ≲ 5.)*

```{r}
#| echo: true
#| output-location: slide
# From your earlier df (n vs #params), compute N-per-parameter
N <- dplyr::n_distinct(long$id)
df2 <- df |>
  tidyr::pivot_wider(names_from = model, values_from = p) |>
  dplyr::mutate(N = N,
         N_per_param_UN = N/UN,
         N_per_param_AR1 = N/AR1,
         N_per_param_CS  = N/CS)
df2
```

---


## UN: implied correlation heatmap

```{r}
#| echo: true
#| output-location: slide
library(reshape2)

# Correlation matrix from UN (first subject — structure is common)
C_un_list <- corMatrix(gls_un$modelStruct$corStruct)
C_un <- if (is.list(C_un_list)) C_un_list[[1]] else C_un_list

dfC <- melt(C_un)
ggplot(dfC, aes(Var1, Var2, fill = value)) +
  geom_tile() + coord_equal() +
  scale_fill_gradient2(limits = c(-1,1)) +
  labs(title = "Implied correlation (UN)", x = "Visit index (1..4)", y = "Visit index (1..4)", fill = "ρ") +
  theme_minimal()
```

---

## UN: residual SD by time (via varIdent)

```{r}
#| echo: true
#| output-location: slide
# varIdent returns relative SD multipliers by factor level (base level=1)
w_coef <- coef(gls_un$modelStruct$varStruct, unconstrained = FALSE)
levs <- levels(long$time_f)
w_all <- setNames(rep(1, length(levs)), levs)
w_all[names(w_coef)] <- w_coef

sigma_eps <- sigma(gls_un)
sd_time <- sigma_eps * w_all

data.frame(time = names(sd_time), sd = as.numeric(sd_time)) %>%
  ggplot(aes(x = time, y = sd, group = 1)) +
  geom_line() + geom_point(size = 2) +
  labs(title = "Estimated residual SD by time (UN + varIdent)",
       x = "Time (weeks)", y = "SD") +
  theme_minimal()
```

---

## UN takeaways

- Flexible; captures **non-constant** variances and **irregular** pairwise correlations.
- Parameter count grows quickly; requires **ample $N$** relative to $n(n+1)/2$.
- Not ideal for **mistimed** visits.

---

## UN failure modes & remedies

- **Failure modes:** non–positive-definite estimates, non-convergence, inflated SEs when $N$ is small, overfit.
- **Common triggers:** too many visits relative to $N$; sparse/mistimed observations; poor starting values.
- **Remedies:**
  - Prefer **structured Σ** (AR(1), Toeplitz, Exponential) with **heterogeneous variances**.
  - Ensure `time_idx` is consecutive integers; set proper UN start vector of length $m(m-1)/2$.
  - Increase optimizer iterations/tolerance; simplify mean model; drop rarely observed times.
  - Consider **mixed-effects** (RI/RS; Chapter 8) to induce realistic Σ with fewer parameters.

---

## Convergence guardrail

- Ensure `time_idx` are **consecutive integers**.
- For UN: start vector length = \(m(m-1)/2\) → `corSymm(value = rep(0.2, p_lower))`.
- Initialize `varIdent` with **numeric** values; not factors/chars.
- Use `glsControl(msMaxIter = 200, tolerance = 1e-8)` if needed.
- If UN unstable: fit **ARH(1)/EXP** first; then UN.

---

## 7.4 Covariance-Pattern Models

- Balance **parsimony** vs **flexibility**.
- Time-series inspired: correlations **decay with lag**.
- Add **heterogeneous variances** via `varIdent(~ 1 | time_f)` when needed.

---


## CS (exchangeable): simple but rigid

- $\Sigma = \sigma^2\{(1-\rho)I + \rho J\}$; constant var, constant corr.
- Often unrealistic (no decay with lag); useful baseline comparator.

```{r}
#| echo: true
#| output-location: slide
summary(gls_cs)$tTable[1:6, , drop = FALSE]
coef(gls_cs$modelStruct$corStruct)  # estimated ρ
```

---

## AR(1): equal spacing, geometric decay

- Corr$(k)=\rho^k$; $k$ = lag in visits; parsimonious (2 params).
- Can **decay too fast**; requires **equal spacing** (use `time_idx`).

```{r}
#| echo: true
#| output-location: slide
summary(gls_ar1)$tTable[1:6, , drop = FALSE]
coef(gls_ar1$modelStruct$corStruct)  # estimated ρ
```

---

## Exponential: unequal spacing (continuous time)

- Corr$(t_j,t_k)=\exp\{-\theta|t_j - t_k|\}$; respects actual gaps.
- Caveats: corr → **1** for replicates at same time (unless **nugget**); corr → **0** at long lags.

```{r}
#| echo: true
#| output-location: slide
summary(gls_exp)$tTable[1:6, , drop = FALSE]
coef(gls_exp$modelStruct$corStruct)  # θ (and nugget if set)
```

---

## Aside: The Nugget Effect - Accounting for Measurement Error

- **Problem:** Models like `corExp` assume `Corr(Yij, Yik) -> 1` as the time gap `|tj - tk| -> 0`. This implies the measurement process is perfect, which is rarely true.
- **What is the "Nugget"?** It's an extra variance component, $\tau^2$, added to the diagonal of the covariance matrix.
  $$ \text{Var}(Y_{ij}) = \sigma^2 + \tau^2 $$

## Aside: The Nugget Effect - Accounting for Measurement Error

- **Interpretation:**
  1.  **Measurement Error:** The inherent variability in assaying or recording a value.
  2.  **Micro-scale Variability:** Biological fluctuations happening on a time scale faster than your measurement interval.
- **Effect:** The correlation for two measurements taken at the *exact same time* is now less than 1:
  $$ \text{Corr}(Y_{ij}, Y_{ik})|_{t_j=t_k} = \frac{\sigma^2}{\sigma^2 + \tau^2} < 1 $$
- **In `nlme`:** Use `correlation = corExp(form = ~ time_num | id, nugget = TRUE)`. This is often a more realistic model.

---

## Heterogeneous variants (common in practice)

- Allow $\operatorname{Var}(Y_{ij})$ to differ by time:
  $$
  \operatorname{Var}(Y_{ij})=\sigma^2 \times \omega_{t(j)}^2,\quad \omega_{t}=1 \ \text{at base level.}
  $$

## Heterogeneous variants (common in practice)

```{r}
#| echo: true
#| output-location: slide
start_rho <- 0.4

# Try GLS with AR(1) + heterogeneous variances
gls_ar1_het <- gls(y ~ group * time_f, data = long,
      correlation = corAR1(value = start_rho, form = ~ time_idx | id),
      weights     = w_time,
      method      = "REML")

coef(gls_ar1_het$modelStruct$corStruct)

AIC(gls_ar1, gls_ar1_het)
```

---

## Implied correlation heatmaps for each Σ  

```{r}
#| echo: true
#| output-location: slide

library(reshape2); library(dplyr); library(ggplot2)

get_cor_mat <- function(fit){
  M <- nlme::corMatrix(fit$modelStruct$corStruct)
  if (is.list(M)) M[[1]] else M
}

mats <- list(CS=gls_cs, AR1=gls_ar1, EXP=gls_exp, UN=gls_un) |>
  purrr::imap(~melt(get_cor_mat(.x)) |> mutate(model=.y))

bind_rows(mats) |>
  ggplot(aes(Var1, Var2, fill=value)) +
  geom_tile() + coord_equal() +
  scale_fill_gradient2(limits=c(-1,1)) +
  facet_wrap(~model, nrow=1) +
  labs(title="Implied correlation by Σ", x="Visit index", y="Visit index", fill="ρ") +
  theme_minimal()

```

---

## Model comparison (AIC/BIC/logLik) including ARH(1)

```{r}
#| echo: true
#| output-location: slide
# Collect models (gls)
mods <- list(
  UN_het   = gls_un,
  CS       = gls_cs,
  AR1      = gls_ar1,
  AR1het   = gls_ar1_het,   # AR(1) + hetero variances
  EXP      = gls_exp,
  EXP_het  = gls_exp_het,
  EXP_nug  = gls_exp_nug
)

data.frame(
  model  = names(mods),
  AIC    = sapply(mods, AIC),
  BIC    = sapply(mods, BIC),
  logLik = sapply(mods, logLik)
) |>
  dplyr::arrange(AIC)
```

---

## Candidate Σ (REML; same fixed effects)

```{r}
#| echo: true
#| output-location: slide
# Independence (for boundary LRT demo)
gls_ind <- gls(y ~ group * time_f, data = long, method = "REML")

# CS, AR(1), Exponential, UN (hetero)
gls_cs  <- gls(y ~ group * time_f, data = long,
               correlation = corCompSymm(value = 0.2, ~ 1 | id), method = "REML")

gls_ar1 <- gls(y ~ group * time_f, data = long,
               correlation = corAR1(value = 0.2, ~ time_idx | id), method = "REML")

gls_exp <- gls(y ~ group * time_f, data = long,
               correlation = corExp(value = 0.2, ~ time_num | id), method = "REML")

gls_un  <- gls(y ~ group * time_f, data = long,
               correlation = corSymm(value = start_vec_un, form = ~ time_idx | id),
               weights     = w_time, method = "REML")
```

---


## how to Add ARH(1) (AR(1) + heterogeneous variances for robustness

```{r}
#| echo: true
#| output-location: slide

start_rho <- 0.4
gls_ar1_het <- gls(y ~ group * time_f, data = long,
                   correlation = corAR1(value = start_rho, form = ~ time_idx | id),
                   weights     = w_time, method = "REML")

```

---

## Compare Σ by REML–AIC/BIC (OK: same mean across models)

```{r}
#| echo: true
#| output-location: slide
mods <- list(
  IND     = gls_ind,
  CS      = gls_cs,
  AR1     = gls_ar1,
  AR1het  = gls_ar1_het,
  EXP     = gls_exp,
  UN_het  = gls_un
)

# AIC/BIC table
data.frame(
  model  = names(mods),
  AIC    = sapply(mods, AIC),
  BIC    = sapply(mods, BIC),
  logLik = sapply(mods, logLik)
) |>
  dplyr::arrange(AIC)
```

---

## Practical guidance for choosing Σ structure

- **Few visits, balanced:** UN-het if $N$ is sufficient.
- **Equal spacing, moderate decay:** AR(1); add `varIdent` if variances differ.
- **Unequal spacing / mistimed:** Exponential (consider **nugget**) or **hybrid** (RI + Exp).
- Sanity-check with implied corr heatmaps, residual diagnostics, and **AIC** for Σ selection.

---

## Aside: Boundary LRT caveat (CS vs Independence; ρ≥0)

- Testing $H_0:\rho=0$ (independence) vs $H_A:\rho>0$ is a **boundary** test.
- Use **mixture $\tfrac12\chi^2_0+\tfrac12\chi^2_1$** (or be conservative with $\alpha=0.10$).
- Using naive χ² tends to over-penalize complex Σ and select overly simple models
- Do not use REML LRTs to compare different mean models.

```{r}
#| echo: true
#| output-location: slide
# REML LRT: IND (reduced) vs CS (full)
LR <- 2 * (as.numeric(logLik(gls_cs)) - as.numeric(logLik(gls_ind)))
df <- 1  # one extra corr parameter
p_standard <- pchisq(LR, df = df, lower.tail = FALSE)           # naive (too big)
p_mixture  <- 0.5 * pchisq(LR, df = df, lower.tail = FALSE)     # correct mixture

data.frame(LR = LR, df = df,
           p_naive_chisq = p_standard,
           p_mixture_50_50 = p_mixture)
```

---

## Aside and Bridge to Ch. 8: CS is a Random Intercept Model

- A model with a random intercept for each subject induces a Compound Symmetry covariance structure.
- We can fit this using `lme()` from the `nlme` package.
- The results for the fixed effects (betas and SEs) will be almost identical to the `gls` fit with `corCompSymm`.

```{r}
#| echo: true
#| output-location: slide
# Fit the random intercept model using lme()
lme_ri <- lme(y ~ group * time_f, random = ~ 1 | id, data = long, method = "REML")

# Compare fixed effects tables
summary(gls_cs)$tTable
summary(lme_ri)$tTable
# Note the tiny differences in SEs/DF due to different optimizers,
# but conceptually they are fitting the same model.
```




## 7.5 Choosing a Covariance Pattern: Strategy & Practice

- Mean and covariance are **interdependent** → choose Σ under a **maximal mean** to avoid spurious residual structure.
- Use **REML** to select among **covariance models** (same fixed effects across candidates).
- Information criteria are comparable only when fixed effects are identical and the estimation method (REML vs ML) is the same across models
- Use **REML LRTs** only for **nested** Σ; mind **boundary** cases (mixture $\chi^2$).

---

## What is a "Maximal" Mean Model and Why Use It?

- **The Problem:** The mean and covariance models are interdependent. A misspecified mean (e.g., assuming a linear trend when it's quadratic) will leave structured patterns in the residuals.
- **The Risk:** We might mistakenly model this leftover structure as part of the covariance, leading to a poor choice for Σ.

---


## What is a "Maximal" Mean Model and Why Use It?

- **The Strategy** 
  1.  Start with a "maximal" mean model that is flexible enough to minimize misspecification. For balanced designs, this is often a **saturated model** (e.g., `group * time_as_factor`).  With many covariates (some continuous): include key main effects + **interactions with time** driven by science; don’t attempt a fully saturated model.
  2.  Use this rich mean model to select the best covariance structure (Σ) using **REML**.
  3.  Once Σ is chosen, you can then simplify the mean model if desired, now using **ML** for comparing different (nested) mean models.
- **Goal:** This separates the task of modeling the mean from modeling the covariance.

 
 
 

## Strengths of Covariance Patterns (Why Use Them?)

- **Parsimony**: Drastically fewer parameters than full UN when $n_\text{time}$ grows (e.g., AR(1), EXP need 1–2 corr parameters).
- **Interpretability**: Patterns (e.g., “correlation decays with lag”) map to scientific intuition.
- **Availability**: Implemented in standard software; easy to compare via **REML–AIC**.
- **Handles missing at planned times** (balanced design with intermittent missingness).

---

## Weaknesses (When They Struggle)

- Many assume **equal spacing** (AR(1), Toeplitz), less suitable for irregular times.
- **Constant-variance** assumption is often wrong in practice ⇒ prefer **heterogeneous variances** over time.
- **UN** explodes in parameters ($n(n+1)/2$) and becomes unstable with modest $n_\text{time}$.
- Some continuous-time models (EXP) imply corr$(t,t)=1$ (no measurement error) and corr $\to 0$ quickly as $\Delta t$ grows — often unrealistic unless a **nugget** (extra variance) is added.

---

## Practical Recommendations

- Use a **maximal mean** (e.g., time as factor × key between-subject covariates).
- Prefer **heterogeneous variances** across time (e.g., `varIdent(~1|time_f)`).
- Compare a **small set**: UN-hetero, AR(1), Exponential (for irregular times).
- Choose by **REML–AIC**; check nested vs UN via **REML–LRT** (mind **boundary** issues).

---

## Decision Flow (Σ)

- Fix **mean**: saturated `group × time_f` (or other maximal mean).
- Try Σ candidates:
  - **UN + hetero-σ** (small $n_\text{time}$).
  - **AR(1)** (equal spacing).
  - **EXP** (irregular spacing).
- Compare **REML–AIC**; for nested vs UN, use **REML–LRT** (use $\alpha=0.10$ to avoid over-parsimonious picks under boundary).
- Refit final model; proceed to inference on $\beta$.

---

## Reporting checklist (what to include)

- **Design & missingness:** planned times, spacing, missingness map.
- **Mean model:** maximal fixed effects used for Σ selection; rationale.
- **Σ candidates & fits:** REML logLik, **AIC**/**BIC** (comparable only with identical fixed effects & estimation method).
- **Nested tests:** REML-LRTs for Σ (note **boundary** mixtures when applicable).
- **Estimated Σ:** key parameters (e.g., $$\rho, \theta$$), varIdent multipliers, implied correlation heatmap.
- **Diagnostics:** residual ACF/semivariogram; convergence notes.
- **Sensitivity:** alternative Σ; final choice justification.


---

## Top 5 Σ Mistakes

- Comparing Σ with different fixed effects (AIC not comparable).

- Using ML for Σ selection (use REML).

- Ignoring boundary mixtures in LRTs.

- Assuming equal spacing for AR(1) when times aren’t equal.

- Treating varIdent coefficients as variances (they’re SD multipliers).

## 7.8 Computing: SAS → R Mapping (Quick Reference)

- SAS `TYPE=UN` ⟶ R: `corSymm(~ time_idx | id)` + `varIdent(~ 1 | time_f)`
- SAS `TYPE=CS` ⟶ R: `corCompSymm(~ 1 | id)` (or random intercept)
- SAS `TYPE=AR(1)` ⟶ R: `corAR1(~ time_idx | id)`
- SAS `TYPE=SP(EXP)(ctime)` ⟶ R: `corExp(~ time_num | id)`

---
